1. env up
  > vagrant up
2. Generate certificate
  > sudo mkdir -p /etc/kubernetes/pki; cd /etc/kubernetes/pki
  Create Cluster root CA
  > openssl genrsa -out ca.key 2048
  > openssl req -x509 -new -nodes -key ca.key -days 10000 -out ca.pem -subj "/CN=kube-ca"
  Create k8s API server keypair
    Replace K8S_SERVICE_IP & MASTER_HOST
  > MASTER_HOST=192.168.205.10
  > K8S_SERVICE_IP=10.3.0.1
  > echo "[req]
req_extensions = v3_req
distinguished_name = req_distinguished_name
[req_distinguished_name]
[ v3_req ]
basicConstraints = CA:FALSE
keyUsage = nonRepudiation, digitalSignature, keyEncipherment
subjectAltName = @alt_names
[alt_names]
DNS.1 = kubernetes
DNS.2 = kubernetes.default
DNS.3 = kubernetes.default.svc
DNS.4 = kubernetes.default.svc.cluster.local
IP.1 = ${K8S_SERVICE_IP}
IP.2 = ${MASTER_HOST}" | sudo tee -a openssl.cnf
  > openssl genrsa -out apiserver.key 2048
  > openssl req -new -key apiserver.key -out apiserver.csr -subj "/CN=kube-apiserver" -config openssl.cnf
  > openssl x509 -req -in apiserver.csr -CA ca.pem -CAkey ca.key -CAcreateserial -out apiserver.pem -days 365 -extensions v3_req -extfile openssl.cnf
  Create k8s worker keypair
  > echo "[req]
req_extensions = v3_req
distinguished_name = req_distinguished_name
[req_distinguished_name]
[ v3_req ]
basicConstraints = CA:FALSE
keyUsage = nonRepudiation, digitalSignature, keyEncipherment
subjectAltName = @alt_names
[alt_names]
IP.1 = \$ENV::WORKER_IP" | sudo tee -a worker-openssl.cnf
    Replace WORKER_FQND & WORKER_IP per node
  > openssl genrsa -out ${WORKER_FQDN}-worker.key 2048
  > WORKER_IP=${WORKER_IP} openssl req -new -key ${WORKER_FQDN}-worker.key -out ${WORKER_FQDN}-worker.csr -subj "/CN=${WORKER_FQDN}" -config worker-openssl.cnf
  > WORKER_IP=${WORKER_IP} openssl x509 -req -in ${WORKER_FQDN}-worker.csr -CA ca.pem -CAkey ca.key -CAcreateserial -out ${WORKER_FQDN}-worker.pem -days 365 -extensions v3_req -extfile worker-openssl.cnf

  > WORKER_FQDN=k8s-worker1
  > WORKER_IP=192.168.205.11
  > openssl genrsa -out ${WORKER_FQDN}-worker.key 2048
  > WORKER_IP=${WORKER_IP} openssl req -new -key ${WORKER_FQDN}-worker.key -out ${WORKER_FQDN}-worker.csr -subj "/CN=${WORKER_FQDN}" -config worker-openssl.cnf
  > WORKER_IP=${WORKER_IP} openssl x509 -req -in ${WORKER_FQDN}-worker.csr -CA ca.pem -CAkey ca.key -CAcreateserial -out ${WORKER_FQDN}-worker.pem -days 365 -extensions v3_req -extfile worker-openssl.cnf
  > WORKER_FQDN=k8s-worker2
  > WORKER_IP=192.168.205.12
  > openssl genrsa -out ${WORKER_FQDN}-worker.key 2048
  > WORKER_IP=${WORKER_IP} openssl req -new -key ${WORKER_FQDN}-worker.key -out ${WORKER_FQDN}-worker.csr -subj "/CN=${WORKER_FQDN}" -config worker-openssl.cnf
  > WORKER_IP=${WORKER_IP} openssl x509 -req -in ${WORKER_FQDN}-worker.csr -CA ca.pem -CAkey ca.key -CAcreateserial -out ${WORKER_FQDN}-worker.pem -days 365 -extensions v3_req -extfile worker-openssl.cnf
  Generate Cluster Admin keypair
  > openssl genrsa -out admin.key 2048
  > openssl req -new -key admin.key -out admin.csr -subj "/CN=kube-admin"
  > openssl x509 -req -in admin.csr -CA ca.pem -CAkey ca.key -CAcreateserial -out admin.pem -days 365
  > sudo chmod 600 *.key


3. Prepare credentials
  > CLUSTER_NAME=k8s-sdchoi
  > CA_CERT=/etc/kubernetes/pki/ca.pem
  > MASTER_IP=192.168.205.10
  > CLI_CERT=/etc/kubernetes/pki/admin.pem
  > CLI_KEY=/etc/kubernetes/pki/admin.key
  > TOKEN=$(dd if=/dev/urandom bs=128 count=1 2>/dev/null | base64 | tr -d "=+/" | dd bs=32 count=1 2>/dev/null)
  > CONTEXT_NAME=default-context
  > USER=admin
  > kubectl config set-cluster $CLUSTER_NAME --certificate-authority=$CA_CERT --embed-certs=true --server=https://$MASTER_IP
  > kubectl config set-credentials $USER --client-certificate=$CLI_CERT --client-key=$CLI_KEY --embed-certs=true --token=$TOKEN
  > kubectl config set-context $CONTEXT_NAME --cluster=$CLUSTER_NAME --user=$USER
  > kubectl config use-context $CONTEXT_NAME
  > sudo cp ~/.kube/config /etc/kubernetes

4. Configure docker
  > sudo systemctl stop docker
  > sudo iptables -t nat -F
  > sudo ip link set docker0 down
  > sudo ip link delete docker0
  > echo "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target docker.socket firewalld.service kubelet.service
Wants=network-online.target
Requires=docker.socket

[Service]
Type=notify
# the default is not to use systemd for cgroups because the delegate issues still
# exists and systemd currently does not support the cgroup feature set required
# for containers run by docker
EnvironmentFile=/etc/default/docker
ExecStart=/usr/bin/dockerd -H fd:// \$DOCKER_OPTS
ExecReload=/bin/kill -s HUP $MAINPID
LimitNOFILE=1000000
# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNPROC=infinity
LimitCORE=infinity
# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0
# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes
# kill only the docker process, not all processes in the cgroup
KillMode=process
# restart the docker process if it exits prematurely
Restart=on-failure
StartLimitBurst=3
StartLimitInterval=60s

[Install]
WantedBy=multi-user.target" | sudo tee -a /etc/systemd/system/docker.service
  > echo "DOCKER_OPTS=\"--bridge=cbr0 --iptables=false --ip-masq=false --mtu=1450\"" |  sudo tee -a /etc/default/docker

5. Configure kubelet
  > echo "# The address for the info server to serve on
KUBELET_ADDRESS=\"--address=0.0.0.0\"

# The port for the info server to serve on
KUBELET_PORT=\"--port=10250\"

# You may leave this blank to use the actual hostname
# Check the node number!
KUBELET_HOSTNAME=\"\"

# Location of the api-server
KUBELET_API_SERVER=\"--api-servers=http://192.168.205.10:8080\"

# Manifests
KUBELET_MANIFESTS=\"--config=/etc/kubernetes/manifests\"

# Add your own!
KUBELET_ARGS=\"--cluster-dns=10.254.0.10 --cluster-domain=cluster.local --configure-cbr0=true --register-node=true\"" | sudo tee -a /etc/kubernetes/kubelet

  > echo "[Unit]
Description=kubelet
Documentation=https://kubernetes.io/docs/admin/kubelet/
After=network.target docker.service

[Service]
EnvironmentFile=/etc/kubernetes/kubelet
ExecStart=/usr/local/bin/kubelet \$KUBELET_ADDRESS \$KUBELET_PORT \$KUBELET_HOSTNAME \$KUBELET_API_SERVER \$KUBELET_MANIFESTS \$KUBELET_ARGS
ExecReload=/bin/kill -s HUP $MAINPID
LimitNPROC=infinity
LimitCORE=infinity
TasksMax=infinity
TimeoutStartSec=0
KillMode=process
Restart=always
StartLimitBurst=3
StartLimitInterval=60s

[Install]
WantedBy=multi-user.target" | sudo tee -a /etc/systemd/system/kubelet.service


6. Configure kube-proxy
  > echo "# The address of the Kubernetes API server
KUBE_MASTER=\"--master=http://192.168.205.10:8080\"

# Add your own!
KUBE_PROXY_ARGS=\"\"" | sudo tee -a /etc/kubernetes/proxy

  > echo "[Unit]
Description=kube-proxy
After=network.target

[Service]
EnvironmentFile=/etc/kubernetes/proxy
ExecStart=/usr/local/bin/kube-proxy \$KUBE_MASTER \$KUBE_PROXY_ARGS
ExecReload=/bin/kill -s HUP \$MAINPID
LimitNPROC=infinity
LimitCORE=infinity
TasksMax=infinity
TimeoutStartSec=0
KillMode=process
Restart=always
StartLimitBurst=3
StartLimitInterval=60s

[Install]
WantedBy=multi-user.target" | sudo tee -a /etc/systemd/system/kube-proxy.service


7. Configure the rests
  > sudo systemctl start kubelet
  > sudo systemctl start docker
  > sudo systemctl start kube-proxy
  > HYPERKUBE_IMAGE=gcr.io/google_containers/hyperkube:v1.6.7
  > echo "{
\"apiVersion\": \"v1\",
\"kind\": \"Pod\",
\"metadata\": {
  \"name\":\"etcd-server1\",
  \"namespace\": \"kube-system\",
  \"annotations\": {
    \"scheduler.alpha.kubernetes.io/critical-pod\": \"\"
  }
},
\"spec\":{
\"hostNetwork\": true,
\"containers\":[
    {
    \"name\": \"etcd-container\",
    \"image\": \"${HYPERKUBE_IMAGE}\",
    \"resources\": {
      \"requests\": {
        \"cpu\": \"200m\"
      }
    },
    \"command\": [
              \"/bin/sh\",
              \"-c\",
              \"if [ -e /usr/local/bin/migrate-if-needed.sh ]; then /usr/local/bin/migrate-if-needed.sh 1>>/var/log/etcd1.log 2>&1; fi; /usr/local/bin/etcd --name etcd-1 --listen-peer-urls http://0.0.0.0:2380 --initial-advertise-peer-urls http://192.168.205.10:2380 --advertise-client-urls http://127.0.0.1:2379 --listen-client-urls http://127.0.0.1:2379 --data-dir /var/etcd/data1 --initial-cluster-state new --initial-cluster etcd-1=http://192.168.205.10:2380 1>>/var/log/etcd1.log 2>&1\"
            ],
    \"env\": [
      { \"name\": \"TARGET_STORAGE\",
        \"value\": \"etcd3\"
      },
      { \"name\": \"TARGET_VERSION\",
        \"value\": \"3.0.17\"
      },
      { \"name\": \"DATA_DIRECTORY\",
        \"value\": \"/var/etcd/data1\"
      }
        ],
    \"livenessProbe\": {
      \"httpGet\": {
        \"host\": \"127.0.0.1\",
        \"port\": 2379,
        \"path\": \"/health\"
      },
      \"initialDelaySeconds\": 15,
      \"timeoutSeconds\": 15
    },
    \"ports\": [
      { \"name\": \"serverport\",
        \"containerPort\": 2380,
        \"hostPort\": 2380
      },
      { \"name\": \"clientport\",
        \"containerPort\": 2379,
        \"hostPort\": 2379
      }
    ],
    \"volumeMounts\": [
      { \"name\": \"varetcd\",
        \"mountPath\": \"/var/etcd\",
        \"readOnly\": false
      },
      { \"name\": \"varlogetcd\",
        \"mountPath\": \"/var/log/etcd1.log\",
        \"readOnly\": false
      },
      { \"name\": \"etc\",
        \"mountPath\": \"/srv/kubernetes\",
        \"readOnly\": false
      }
    ]
    }
],
\"volumes\":[
  { \"name\": \"varetcd\",
    \"hostPath\": {
        \"path\": \"/mnt/master-pd/var/etcd\"}
  },
  { \"name\": \"varlogetcd\",
    \"hostPath\": {
        \"path\": \"/var/log/etcd1.log\"}
  },
  { \"name\": \"etc\",
    \"hostPath\": {
        \"path\": \"/etc/kubernetes\"}
  }
]
}}" | sudo tee -a /etc/kubernetes/manifests/etcd.manifest


  > echo "{
  \"kind\": \"Pod\",
  \"apiVersion\": \"v1\",
  \"metadata\": {
    \"name\": \"kube-apiserver\",
    \"namespace\": \"kube-system\"
  },
  \"spec\": {
    \"hostNetwork\": true,
    \"containers\": [
      {
        \"name\": \"kube-apiserver\",
        \"image\": \"${HYPERKUBE_IMAGE}\",
        \"command\": [
                  \"/hyperkube\",
                  \"apiserver\",
                  \"--insecure-bind-address=127.0.0.1\",
                  \"--service-cluster-ip-range=10.0.0.0/16\",
                  \"--etcd-servers=http://127.0.0.1:2379\",
                  \"--tls-cert-file=/srv/kubernetes/apiserver.pem\",
                  \"--tls-private-key-file=/srv/kubernetes/apiserver.key\",
                  \"--admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,PersistentVolumeLabel,DefaultStorageClass,ResourceQuota,DefaultTolerationSeconds\"
        ],
        \"ports\": [
          {
            \"name\": \"https\",
            \"hostPort\": 443,
            \"containerPort\": 443
          },
          {
            \"name\": \"local\",
            \"hostPort\": 8080,
            \"containerPort\": 8080
          }
        ],
        \"volumeMounts\": [
          {
            \"name\": \"srvkube\",
            \"mountPath\": \"/srv/kubernetes\",
            \"readOnly\": true
          }
        ],
        \"livenessProbe\": {
          \"httpGet\": {
            \"scheme\": \"HTTP\",
            \"host\": \"127.0.0.1\",
            \"port\": 8080,
            \"path\": \"/healthz\"
          },
          \"initialDelaySeconds\": 15,
          \"timeoutSeconds\": 15
        }
      }
    ],
    \"volumes\": [
      {
        \"name\": \"srvkube\",
        \"hostPath\": {
          \"path\": \"/etc/kubernetes/pki\"
        }
      }
    ]
  }
}" | sudo tee -a /etc/kubernetes/manifests/kube-apiserver.manifest


  > echo "{
  \"kind\": \"Pod\",
  \"apiVersion\": \"v1\",
  \"metadata\": {
    \"name\": \"kube-scheduler\",
    \"namespace\": \"kube-system\"
  },
  \"spec\": {
    \"hostNetwork\": true,
    \"containers\": [
      {
        \"name\": \"kube-scheduler\",
        \"image\": \"${HYPERKUBE_IMAGE}\",
        \"command\": [
          \"/hyperkube\",
          \"scheduler\",
          \"--master=127.0.0.1:8080\"
        ],
        \"livenessProbe\": {
          \"httpGet\": {
            \"scheme\": \"HTTP\",
            \"host\": \"127.0.0.1\",
            \"port\": 10251,
            \"path\": \"/healthz\"
          },
          \"initialDelaySeconds\": 15,
          \"timeoutSeconds\": 15
        }
      }
    ]
  }
}" | sudo tee -a /etc/kubernetes/manifests/kube-scheduler.manifest


  > echo "{
  \"kind\": \"Pod\",
  \"apiVersion\": \"v1\",
  \"metadata\": {
    \"name\": \"kube-controller-manager\",
    \"namespace\": \"kube-system\"
  },
  \"spec\": {
    \"hostNetwork\": true,
    \"containers\": [
      {
        \"name\": \"kube-controller-manager\",
        \"image\": \"$HYPERKUBE_IMAGE\",
        \"command\": [
          \"/hyperkube\",
          \"controller-manager\",
          \"--cluster-name=k8s-sdchoi\",
          \"--cluster-cidr=10.244.0.0/16\",
          \"--cluster-signing-cert-file=/srv/kubernetes/ca.pem\",
          \"--cluster-signing-key-file=/srv/kubernetes/ca.key\",
          \"--service-account-private-key-file=/srv/kubernetes/apiserver.key\",
          \"--root-ca-file=/srv/kubernetes/apiserver.pem\",
          \"--master=127.0.0.1:8080\"
        ],
        \"volumeMounts\": [
          {
            \"name\": \"srvkube\",
            \"mountPath\": \"/srv/kubernetes\",
            \"readOnly\": true
          },
          {
            \"name\": \"etcssl\",
            \"mountPath\": \"/etc/ssl\",
            \"readOnly\": true
          }
        ],
        \"livenessProbe\": {
          \"httpGet\": {
            \"scheme\": \"HTTP\",
            \"host\": \"127.0.0.1\",
            \"port\": 10252,
            \"path\": \"/healthz\"
          },
          \"initialDelaySeconds\": 15,
          \"timeoutSeconds\": 15
        }
      }
    ],
    \"volumes\": [
      {
        \"name\": \"srvkube\",
        \"hostPath\": {
          \"path\": \"/etc/kubernetes/pki\"
        }
      },
      {
        \"name\": \"etcssl\",
        \"hostPath\": {
          \"path\": \"/etc/ssl\"
        }
      }
    ]
  }
}"  | sudo tee -a /etc/kubernetes/manifests/kube-controller-manager.manifest


8. Configure kubectl (on master node)
  > chmod +x kubectl; sudo chown root:root kubectl
  > sudo mv kubectl /usr/local/bin
  > export KUBECONFIG=/etc/kubernetes/config
  > kubectl get nodes

9. Deploy kube-dns
  > for item in Makefile kubedns-controller.yaml.base kubedns-svc.yaml.base transforms2salt.sed transforms2sed.sed; do
    wget https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dns/$item
done
  > DNS_SERVER_IP=10.254.0.10
  > DNS_DOMAIN=cluster.local
  > cat <<EOF > transforms2sed.sed
s/__PILLAR__DNS__SERVER__/$DNS_SERVER_IP/g
s/__PILLAR__DNS__DOMAIN__/$DNS_DOMAIN/g
/__PILLAR__FEDERATIONS__DOMAIN__MAP__/d
s/__MACHINE_GENERATED_WARNING__/Warning: This is a file generated from the base underscore template file: __SOURCE_FILENAME__/g
EOF
  > sed -f transforms2salt.sed kubedns-controller.yaml.base | sed s/__SOURCE_FILENAME__/kubedns-controller.yaml.base/g > kubedns-controller.yaml.in
  > sed -f transforms2salt.sed kubedns-svc.yaml.base | sed s/__SOURCE_FILENAME__/kubedns-svc.yaml.base/g > kubedns-svc.yaml.in
  > sed -f transforms2sed.sed kubedns-controller.yaml.base  | sed s/__SOURCE_FILENAME__/kubedns-controller.yaml.base/g > kubedns-controller.yaml.sed
  > sed -f transforms2sed.sed kubedns-svc.yaml.base  | sed s/__SOURCE_FILENAME__/kubedns-svc.yaml.base/g > kubedns-svc.yaml.sed
  > sed -i 's/serviceAccountName: kube-dns/serviceAccountName: default/g' kubedns-controller.yaml.sed
  > kubectl create -f kubedns-controller.yaml.sed
  > kubectl create -f kubedns-svc.yaml.sed
  > kubectl get all --all-namespaces
  > sed -i 's/KUBELET_ARGS=\"\"/KUBELET_ARGS=\"--cluster_dns=10.254.0.10 --cluster_domain=cluster.local\"/g' /etc/kubernetes/kubelet
  > sudo systemctl restart kubelet; sudo systemctl status kubelet




. Configure flannel on each machine
  > vagrant ssh k8s-master
  > tar -xvzf flanneld.tgz
  > sudo mv flanneld /usr/local/bin; sudo chown root:root /usr/local/bin/flanneld
  > curl http://127.0.0.1:2379/v2/keys/coreos.com/network/config -XPUT -d value="{\"Network\":\"10.0.0.0/8\",\"SubnetLen\":24,\"Backend\":{\"Type\":\"vxlan\",\"Port\":8432}}"
  > echo "[Unit]
Description=flanneld
Documentation=https://coreos.com/flanneld/docs/latest
After=etcd.service

[Service]
ExecStart=/usr/local/bin/flanneld \
			-iface=192.168.205.10
ExecReload=/bin/kill -s HUP $MAINPID
LimitNPROC=infinity
LimitCORE=infinity
TasksMax=infinity
TimeoutStartSec=0
KillMode=process
Restart=always
StartLimitBurst=3
StartLimitInterval=60s

[Install]
WantedBy=multi-user.target" | sudo tee -a /etc/systemd/system/flanneld.service
  > sudo systemctl daemon-reload; sudo systemctl start flanneld
  > exit
